\newpage
\section{Related Work}

Automatic music transcription (AMT) has been attempted since the 1970s and polyphonic music transcription dates to the 1990s \cite{REF:1}
\par

\subsection{State of the art in AMT}

There has been substantial progress made in the field of AMT. Neural networks, in particular, have met and surpassed the performance of traditional pitch recognition techniques on polyphonic audio.
\par

A model used in \cite{REF:2} uses 87 Support Vector Machine (SVM) classifiers to perform frame-level classification with the advantage of simplicity, and then a Hidden Markov Model (HMM) post-processing was adopted to smooth the results. On top of it, Deep Belief Network(DBN) was added to learn higher layer representation of features in \cite{REF:3}. Since none of the approaches has reached the same level of accuracy as human experts, most music transcription work is completed by musicians.
\par

The first major AMT work is Smaragdis et al.\cite{REF:8}. This approach uses Non-Negative Matrix Factorization (NMF). This is the main methodology employed in software for automatic transcription, but it has it's limitations. For example, it needs to know how many individual notes are desired for the transcription (information that is not always available).
\par

The next work worth mentioning is Emiya et al.\cite{REF:9}, not because of their transcription system (as it was out-performed in the same year), but because of the dataset they created that has become the standard in evaluating any multi-pitch estimation system. They created the MIDI-Aligned Piano Sounds (MAPS) data set composed of around 10,000 piano sounds either recorded by using an upright Disklavier piano or generated by several virtual piano software products based on sampled sounds. The dataset
consists of audio and corresponding annotations for isolated sounds, chords, and complete pieces of piano music.
\par

Melodyne is a popular plugin used for Music Transcription and Pitch Correction. It costs up to \$700. 	
The Melodic and Polyphonic algorithms offer you, in the case of vocals as well as both mono- and polyphonic instruments, full access to the notes of which the sound is composed as well as to their musical parameters.
There's no public information about what approach they used.
\par

\subsection{NMF approach to AMT}
The work of Smaragdis et al.\cite{REF:8} was the first major AMT work. This approach used NMF. It works by factoring a non-negative matrix \textbf{X} $\in R^{\geq 0, MxN}$ into two factor matrices: \textbf{H} $\in R^{\geq 0, MxR}$ and \textbf{W} $\in R^{\geq 0, RxN}$, where R is chosen, in the case of a piano roll it is 88. \textbf{H} represents when each component is active, known as the Activation Matrix and \textbf{W} shows the frequency spectrum of what should only be a single note, known as Basis Matrix. \cite{NMF}
\par

\begin{figure}[h]
	\caption[Non-negative matrix factorization]{Non-negative matrix factorization \cite{NMF_figure}}
	\centering
	\includegraphics[width=1\textwidth, height=\textheight, keepaspectratio]{"resources/NMF"}
\end{figure}

While this is the main technique used in software for AMT, it has it's limitations. It needs to know how many individual notes are desired for the transcription, something which is not always available.

\subsection{CNN approach to AMT}
With the development of deep learning in recent years, researchers were inspired to apply neural networks to achieve AMT. Many models were proposed, including CNNs, Recurrent neural networks (RNN), Long-short term memory networks (LSTM) \cite{REF:1} \cite{REF:5}. Five models were compared and CNNS were reported to have the best performance. \cite{REF:7} 
\par

Sigtia et al.\cite{REF:10} built the first AMT system using CNN, outperforming the state of the art approaches using NMF. Convolutional
Neural Networks are a discriminative approach to AMT, which has been found to be a
viable alternative to spectrogram factorization techniques. Discriminative approaches
aim to directly classify features extracted from frames of audio to the output pitches.
This approach uses complex classifiers that are trained using large amounts of training
data to capture the variability in the inputs, instead of constructing an instrument
specific model. 
\par

\begin{table} [h!]
	\centering
	\caption{Optimal parameters and architecture for ConvNet in Sigtia et al \cite{REF:7}}
	\begin{tabular}{ |c|c| } 
		\hline
		Parameter Name & Value \\ \hline
		Window Size (Spectrogram Frames) &  7 \\ \hline
		Number of ConvNet Layers (Conv+tanh+Pooling) & 2 \\ \hline
		Number of Fully Connected Layers & 2 \\ \hline
		Window Shapes(1,2) & (5,25),(3,5) \\ \hline
		Pooling Size & (1,3) \\ \hline
		Fully Connected Widths (1,2) & 1000,200 \\
		\hline
	\end{tabular}
	\label{table:sigtia}
\end{table}

Sigtia et al. \cite{REF:10} explored various models for pitch detection. In addition to CNNs, they tried Deep Neural Networks and Recurrent Neural Networks. The results have shown that their CNN based model outperformed the others for this task. In their paper, they propose a Music Language Model (MLM) that's based on RNNs in order to handle the polyphonic musical data. \cite{benetos}
\par

There are some products on the market, AnthemScore for example, that use CNNs for AMT. They approach note detection as an image recognition problem by creating spectrograms of the audio. They show how the spectrum or frequency content changes over time. The method used for creating the spectrograms is the constant Q transform instead of the more common Short Time Fourier Transform (STFT) method.
\par



