\newpage
\chapter{Related Work}

Automatic music transcription (AMT) has been attempted since the 1970s and polyphonic music transcription dates to the 1990s~\cite{REF:1}.
\par

\section{State of the art in AMT}

Substantial progress has been made in the field of AMT. Neural networks, in particular, have met and surpassed the performance of traditional pitch recognition techniques on polyphonic audio.
\par

Poliner et al.~\cite{REF:2} proposed a solution that uses 87 Support Vector Machine (SVM) classifiers to perform frame-level classification with the advantage of simplicity, and then a Hidden Markov Model (HMM) post-processing was adopted to smooth the results. On top of it, Nam et al.~\cite{REF:3} added a Deep Belief Network(DBN) to learn a higher layer representation of features. Since none of the approaches has reached the accuracy of human experts, most music transcription work is completed by musicians.
\par

The first major AMT approach was proposed by Smaragdis et al.~\cite{REF:8}. It uses Non-Negative Matrix Factorization (NMF). This is the main methodology employed in software for automatic transcription, but it has its limitations. For example, it needs to know how many individual notes are desired for the transcription (information that is not always available).
\par

The next work worth mentioning is the one of Emiya et al.~\cite{REF:9}, not because of their transcription system (as it was out-performed in the same year), but because of the dataset they created that has become the standard in evaluating any multi-pitch estimation system. They created the MIDI-Aligned Piano Sounds (MAPS) data set composed of around 10,000 piano sounds either recorded by using an upright Disklavier piano or generated by several virtual piano software products based on sampled sounds. The dataset
consists of audio and corresponding annotations for isolated sounds, chords, and complete pieces of piano music.
\par

Melodyne is a popular plugin used for Music Transcription and Pitch Correction. It costs up to \$700. 	
The Melodic and Polyphonic algorithms offer, in the case of vocals as well as both mono- and polyphonic instruments, full access to the notes of which the sound is composed as well as to their musical parameters.
There's no public information about what approach they used.
\par

\section{NMF approach to AMT}
The work of Smaragdis et al.~\cite{REF:8} was the first major AMT work. This approach used NMF. It works by factoring a non-negative matrix $X$ $\in R^{\geq 0, MxN}$ into two-factor matrices: $H$ $\in R^{\geq 0, MxR}$ and $W$ $\in R^{\geq 0, RxN}$, where $R$ is chosen, in the case of a piano roll it is 88. $H$ represents when each component is active, known as the Activation Matrix and $W$ shows the frequency spectrum of what should only be a single note, known as Basis Matrix~\cite{NMF}.
\par

\begin{figure}[h]
	\caption[Non-negative matrix factorization]{Non-negative matrix factorization~\cite{NMF_figure}}
	\centering
	\includegraphics[width=1\textwidth, height=\textheight, keepaspectratio]{"resources/NMF"}
\end{figure}

While this is the main technique used in software for AMT, it has its limitations. It needs to know how many individual notes are desired for the transcription, something which is not always available.

\section{CNN approach to AMT}
With the development of deep learning in recent years, researchers were inspired to apply neural networks to achieve AMT. Many models were proposed, including CNNs, Recurrent neural networks (RNN), and Long-short term memory networks (LSTM)~\cite{REF:1}~\cite{REF:5}. Five models were compared and CNN was reported to have the best performance~\cite{REF:7}. 
\par

Sigtia et al.~\cite{REF:10} built the first AMT system us	ing CNN, outperforming the state of the art approaches using NMF. Convolutional
Neural Networks are a discriminative approach to AMT, which has been found to be a
viable alternative to spectrogram factorization techniques. Discriminative approaches
aim to directly classify features extracted from frames of audio to the output pitches.
This approach uses complex classifiers that are trained using large amounts of training
data to capture the variability in the inputs, instead of constructing an instrument-specific model. 
\par

\begin{table} [h!]
	\centering
	\caption{Optimal parameters and architecture for ConvNet in Sigtia et al.~\cite{REF:7}}
	\begin{tabular}{ |c|c| } 
		\hline
		Parameter Name & Value \\ \hline
		Window Size (Spectrogram Frames) &  7 \\ \hline
		Number of ConvNet Layers (Conv+tanh+Pooling) & 2 \\ \hline
		Number of Fully Connected Layers & 2 \\ \hline
		Window Shapes(1,2) & (5,25),(3,5) \\ \hline
		Pooling Size & (1,3) \\ \hline
		Fully Connected Widths (1,2) & 1000,200 \\
		\hline
	\end{tabular}
	\label{table:sigtia}
\end{table}

Sigtia et al..~\cite{REF:10} explored various models for pitch detection. In addition to CNNs, they tried Deep Neural Networks and Recurrent Neural Networks. The results have shown that their CNN based model outperformed the others for this task. In their paper, they propose a Music Language Model (MLM) that's based on RNNs in order to handle the polyphonic musical data~\cite{benetos}.
\par

There are some products on the market (i.e. AnthemScore), that use CNNs for AMT. They approach note detection as an image recognition problem by creating spectrograms of the audio. They show how the spectrum or frequency content changes over time. The method used for creating the spectrograms is the constant Q transform instead of the more common Short Time Fourier Transform (STFT) method.
\par



