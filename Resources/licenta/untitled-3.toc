\contentsline {section}{\numberline {1}Theoretical Background}{4}
\contentsline {subsection}{\numberline {1.1}Introduction to Deep Learning}{4}
\contentsline {paragraph}{ Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to effectively perform a specific task without using explicit instructions, relying on patterns and inference instead. Machine learning algorithms build a mathematical model of sample data, known as "training data", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in a wide variety of applications, such as email filtering, and computer vision, where it is infeasible to develop an algorithm of specific instructions for performing the task. The represantation of data fed into a ML algorithm plays a major role, as it affects the algorithm\IeC {\textquoteright }s ability to efficiently extract signals and make decisions. Thus, it is important to carefully select the information included in such a representation. Formally, the representation is com- posed of multiple features extracted from raw data. The process of creating new features requires good and intuitive understanding of the data at hand, becoming incrementally time-consuming with the sophistication of the new features. Thus, the biggest challenge of handcrafted features is deciding which features are important and relevant to the problem \cite {Goodfellow-et-al-2016} }{4}
\contentsline {subsection}{\numberline {1.2}Neural Networks}{4}
\contentsline {paragraph}{ This section introduces the main concepts related to neural networks. Neural networks have been around since the 1940s and could initially handle only one hidden layer. But with the development of technologies and hardware it became possible to build deeper, more effective architectures, which leads to deep learning as we know it today. }{4}
\contentsline {subsubsection}{\numberline {1.2.1}Brief History}{4}
\contentsline {paragraph}{ At first, neural networks were inspired by how the biological brain works, which is why deep learning was also called artificial neural networks (ANNs)\cite {Goodfellow-et-al-2016}. In biology, a neuron is the cell that receives, processes and transmits information to other neurons through connections called synapses \cite {neuron}. On the other hand, artificial neurons are defined as computational units (usually mathematical functions) that take one or more inputs and generate an output. McCulloch and Pits designed an initial version of the neuron as a linear model in 1943, aiming to replicate brain function \cite {REF:11}: }{4}
\contentsline {subsubsection}{\numberline {1.2.2}Components of an artificial neural network}{5}
\contentsline {paragraph}{ A simple artificial neural network (ANN) consists of input layer, hidden layer and output layer, where the values of the hidden layer are used as inputs for the output layer. A network with several layers is known as a deep neural network. Data flows through the neurons of the layer. Each neuron transforms the input it receives and sends it to the next layer. The neurons share the same characteristics regardless of the layer they are part of. }{5}
\contentsline {paragraph}{ The Neuron, also called node, is the basic unit of a neural network. It's main components include inputs, weights, activation function and output(s). From a high level point of view the inputs are multiplied by weights, then an activation function is applied to the result and finally, another function computes the output\cite {REF:12}\cite {REF:13}. }{5}
\contentsline {subsubsection}{\numberline {1.2.3}Underfitting and Overfitting}{7}
\contentsline {paragraph}{ Neural networks are able to learn complicated non-linear functions to fit any training set. On the downside, this may lead to overfitting where the neural network learns the training data so well that it is unable to generalize on new, unseen data. This problem can especially occur on datasets with a small amount of data to learn from. }{7}
\contentsline {paragraph}{ Underfitting, the counterpart of overfitting, happens when a machine learning model isn\IeC {\textquoteright }t complex enough to accurately capture relationships between a dataset\IeC {\textquoteright }s features and a target variable. An underfitted model results in problematic outcomes on new data, or data that it wasn\IeC {\textquoteright }t trained on, and many times performs poorly even on training data. }{7}
\contentsline {subsection}{\numberline {1.3}Convolutional Neural Networks}{8}
\contentsline {subsubsection}{\numberline {1.3.1}Find subsections}{8}
\contentsline {subsection}{\numberline {1.4}How Music is Made}{9}
\contentsline {subsubsection}{\numberline {1.4.1}Pitch}{9}
\contentsline {subsubsection}{\numberline {1.4.2}Rhythm}{9}
\contentsline {subsection}{\numberline {1.5}Music Transcription}{10}
\contentsline {subsubsection}{\numberline {1.5.1}Traditional Music Transcription}{10}
\contentsline {subsubsection}{\numberline {1.5.2}Automated Music Transcription}{10}
\contentsline {section}{\numberline {2}Related Work}{11}
\contentsline {paragraph}{ Automatic music transcription (AMT) has been attempted since the 1970s and polyphonic music transcription dates to the 1990s \cite {REF:1} }{11}
\contentsline {subsection}{\numberline {2.1}State of the art in AMT}{11}
\contentsline {paragraph}{ A model used in \cite {REF:2} uses 87 Support Vector Machine (SVM) classifiers to perform frame-level classification with the advantage of simplicity, and then a Hidden Markov Model (HMM) post-processing was adopted to smooth the results. On top of it, Deep Belief Network(DBN) was added to learn higher layer representation of features in \cite {REF:3}. Since none of the approaches has reached the same level of accuracy as human experts, most music transcription work is completed by musicians. With the development of deep learning in recent years, many researchers were inspired to apply networks to accomplish AMT. A model based on Convolutional Neural Networks (CNN) was proposed in \cite {REF:4}. More models adopted Reccurent Neural Networks (RNN) or Long Short-Term Memory (LSTM) due to its capability of dealing with sequential data \cite {REF:1} \cite {REF:5} \cite {REF:6}. In \cite {REF:7}, 5 models were compared and the ConvNet model was reported as resulting in the best performance. }{11}
\contentsline {paragraph}{ The first majort AMT work is Smaragdis et al.\cite {REF:8}. This approach uses Non-Negative Matrix Factorization (NMF). This is the main methodology employed in software for automatic transcription, but it has it's limitations. For example, it needs to know how many individual notes are desired for the transcription (information that is not always available). }{11}
\contentsline {paragraph}{ The next work worth mentioning is Emiya et al.\cite {REF:9}, not because of their transcription system (as it was out-performed in the same year), but because of the dataset they created that has become the standard in evaluating any multi-pitch estimation system. They created the MIDI-Aligned Piano Sounds (MAPS) data set composed of around 10,000 piano sounds either recorded by using an upright Disklavier piano or generated by several virtual piano software products based on sampled sounds. The dataset consists of audio and corresponding annotations for isolated sounds, chords, and complete pieces of piano music. For our purpose we use only the isolated sounds (daca nu gasesc ceva mai bun). }{11}
\contentsline {paragraph}{ Sigtia et al.\cite {REF:10} built the first AMT system using CNN, outperforming the state-of-the-art approaches using NMF. Convolutional Neural Networks are a discriminative approach to AMT, which has been found to be a viable alternative to spectrogram factorization techniques. Discriminative approaches aim to directly classify features extracted from frames of audio to the output pitches. This approach uses complex classifiers that are trained using large amounts of training data to capture the variability in the inputs, instead of constructing an instrument specific model. }{11}
\contentsline {subsection}{\numberline {2.2}Products}{13}
\contentsline {paragraph}{ In this section, we present products that use deep learning for AMT. }{13}
\contentsline {subsubsection}{\numberline {2.2.1}Melodyne}{13}
\contentsline {paragraph}{ Melodyne is a popular plugin used for Music Transcription and Pitch Correction. It costs up to \$700. }{13}
\contentsline {paragraph}{ The Melodic and Polyphonic algorithms offer you, in the case of vocals as well as both mono- and polyphonic instruments, full access to the notes of which the sound is composed as well as to their musical parameters. }{13}
\contentsline {subsubsection}{\numberline {2.2.2}AnthemScore}{13}
\contentsline {paragraph}{ AnthemScore is a product used for Music Transcription that uses CNN. }{13}
\contentsline {paragraph}{ They approach note detection as an image recognition problem by creating spectrograms of the audio. They show how the spectrum or frequency content changes over time. The method used for creating the spectrograms is the constant Q transform instead of the more common Short Time Fourier Transform (STFT) method. }{13}
\contentsline {section}{References}{14}
