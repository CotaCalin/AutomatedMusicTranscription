\relax 
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{neuron}
\citation{REF:11}
\@writefile{toc}{\contentsline {section}{\numberline {1}Theoretical Background}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Introduction to Deep Learning}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Neural Networks}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Brief History}{4}\protected@file@percent }
\citation{REF:12}
\citation{REF:13}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Components of an artificial neural network}{5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Common activation functions}}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3}Underfitting and Overfitting}{7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Example of overfitting and underfitting}}{7}\protected@file@percent }
\citation{cnn}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Convolutional Neural Networks}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Find subsections}{8}\protected@file@percent }
\citation{physics_of_sound}
\citation{speed_of_sound}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Sound}{9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Traveling Wave}}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Pitch}{9}\protected@file@percent }
\citation{dsp}
\citation{discrete}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Digital Signal Processing}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.1}Discrete Fourier Transformation}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Music Transcription}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.1}Traditional Music Transcription}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.2}Automated Music Transcription}{11}\protected@file@percent }
\citation{REF:1}
\citation{REF:2}
\citation{REF:3}
\citation{REF:4}
\citation{REF:1}
\citation{REF:5}
\citation{REF:6}
\citation{REF:7}
\citation{REF:8}
\citation{REF:9}
\citation{REF:10}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Automatic music transcription (AMT) has been attempted since the 1970s and polyphonic music transcription dates to the 1990s \cite  {REF:1} }{12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}State of the art in AMT}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ A model used in \cite  {REF:2} uses 87 Support Vector Machine (SVM) classifiers to perform frame-level classification with the advantage of simplicity, and then a Hidden Markov Model (HMM) post-processing was adopted to smooth the results. On top of it, Deep Belief Network(DBN) was added to learn higher layer representation of features in \cite  {REF:3}. Since none of the approaches has reached the same level of accuracy as human experts, most music transcription work is completed by musicians. With the development of deep learning in recent years, many researchers were inspired to apply networks to accomplish AMT. A model based on Convolutional Neural Networks (CNN) was proposed in \cite  {REF:4}. More models adopted Reccurent Neural Networks (RNN) or Long Short-Term Memory (LSTM) due to its capability of dealing with sequential data \cite  {REF:1} \cite  {REF:5} \cite  {REF:6}. In \cite  {REF:7}, 5 models were compared and the ConvNet model was reported as resulting in the best performance. }{12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ The first majort AMT work is Smaragdis et al.\cite  {REF:8}. This approach uses Non-Negative Matrix Factorization (NMF). This is the main methodology employed in software for automatic transcription, but it has it's limitations. For example, it needs to know how many individual notes are desired for the transcription (information that is not always available). }{12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ The next work worth mentioning is Emiya et al.\cite  {REF:9}, not because of their transcription system (as it was out-performed in the same year), but because of the dataset they created that has become the standard in evaluating any multi-pitch estimation system. They created the MIDI-Aligned Piano Sounds (MAPS) data set composed of around 10,000 piano sounds either recorded by using an upright Disklavier piano or generated by several virtual piano software products based on sampled sounds. The dataset consists of audio and corresponding annotations for isolated sounds, chords, and complete pieces of piano music. For our purpose we use only the isolated sounds (daca nu gasesc ceva mai bun). }{12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Sigtia et al.\cite  {REF:10} built the first AMT system using CNN, outperforming the state of the art approaches using NMF. Convolutional Neural Networks are a discriminative approach to AMT, which has been found to be a viable alternative to spectrogram factorization techniques. Discriminative approaches aim to directly classify features extracted from frames of audio to the output pitches. This approach uses complex classifiers that are trained using large amounts of training data to capture the variability in the inputs, instead of constructing an instrument specific model. }{12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Products}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ In this section, we present products that use deep learning for AMT. }{14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Melodyne}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Melodyne is a popular plugin used for Music Transcription and Pitch Correction. It costs up to \$700. }{14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ The Melodic and Polyphonic algorithms offer you, in the case of vocals as well as both mono- and polyphonic instruments, full access to the notes of which the sound is composed as well as to their musical parameters. }{14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}AnthemScore}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ AnthemScore is a product used for Music Transcription that uses CNN. }{14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ They approach note detection as an image recognition problem by creating spectrograms of the audio. They show how the spectrum or frequency content changes over time. The method used for creating the spectrograms is the constant Q transform instead of the more common Short Time Fourier Transform (STFT) method. }{14}\protected@file@percent }
\bibdata{references}
\bibcite{Goodfellow-et-al-2016}{1}
\bibcite{neuron}{2}
\bibcite{REF:11}{3}
\bibcite{REF:12}{4}
\bibcite{REF:13}{5}
\bibcite{cnn}{6}
\bibcite{physics_of_sound}{7}
\bibcite{speed_of_sound}{8}
\bibcite{dsp}{9}
\bibcite{discrete}{10}
\bibcite{REF:1}{11}
\bibcite{REF:2}{12}
\bibcite{REF:3}{13}
\bibcite{REF:4}{14}
\bibcite{REF:5}{15}
\@writefile{toc}{\contentsline {section}{References}{15}\protected@file@percent }
\bibcite{REF:6}{16}
\bibcite{REF:7}{17}
\bibcite{REF:8}{18}
\bibcite{REF:9}{19}
\bibcite{REF:10}{20}
\bibstyle{ieeetr}
