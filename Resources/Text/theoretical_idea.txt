2.1 Introduction to deep learning

This section briefly introduces the concept of deep learning.
  Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to effectively perform a specific task without using explicit instructions, relying on patterns and inference instead. Machine learning algorithms build a mathematical model of sample data, known as "training data", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in a wide variety of applications, such as email filtering, and computer vision, where it is infeasible to develop an algorithm of specific instructions for performing the task. The represantation of data fed into a ML algorithm plays a major role, as it affects the algorithm’s ability to efficiently extract signals and make decisions. Thus, it is important to carefully select the information included in such a representation. Formally, the representation is com- posed of multiple features extracted from raw data. The process of creating new features requires good and intuitive understanding of the data at hand, becoming incrementally time-consuming with the sophistication of the new features. Thus, the biggest challenge of handcrafted features is deciding which features are important and relevant to the problem [Goodfellow et al., 2016].
  
  
  2.2 Neural Networks
    This section introduces the main concepts related to neural networks. Neural networks have been around since the 1940s and could initially handle only one hidden layer. But with the development of technologies and hardware it became possible to build deeper, more effective architectures, which leads to deep learning as we know it today. This makes the study of neural networks in our context an interesting starting point, before diving deeper into concepts related to NLP.
 
  2.2.1 Brief History
  Initially, neural networks were inspired by how the biological brain works, which is why deep learning was also called artificial neural networks (ANNs) [Goodfellow et al., 2016]. In biology, a neuron is the cell that receives, processes and transmits information to other neurons through connections called synapses [neu, 2018]. On the other hand, artificial neurons are defined as computational units (usually mathematical functions) that take one or more inputs and generate an output.
McCulloch and Pits designed an initial version of the neuron as a linear model in 1943, aiming to replicate brain function [McCulloch and Pitts, 1943]:
  f(x,w)=x1w1 +x2w2 +...+xnwn
  where x1, ..., xn are the input values and w1, ..., w2 is a set of hand-chosen weights.
  
  2.2.2 Components of an artificial neuron
  A simple artificial neural network (ANN) consists of input layer, hidden layer and output layer, where the values of the hidden layer are used as inputs for the output layer. A network with several layers is known as a deep neural network. Data flows through the neurons of the layers with each neuron transforming the input it receives and forwarding it to the next layer. The neurons share the same characteristics irrespective of the layer they are part of.
The main components of an artificial neuron include inputs, weights, activation func- tion and output(s). On the high-level, the inputs are multiplied by weights, then an activation function is applied to the result and finally, another function computes the output. The components of an artificial neuron are [Bangal, 2009]:
  
  • Weights are defined as adaptive coefficients, whose values are changed during the learning process. Each input of a neuron is multiplied by a relative weighting factor, which decides the impact it will have further in the computation.
  
  • Summation function helps combine the input and weights in different ways, before passing the result to the activation function. Denote the input as x = [x1, x2, ...xn] and weight vector as W = [w1, w2, ...wn]. Thus, the summation func- tion could be defined as the dot product between these two vectors:
  xT W =x1 ·w1 +x2 ·w2 +...+xn ·wn
  
  In addition, the summation function could instead compute the minimum, maxi- mum, product etc. depending on the designated network architecture. To gener- alize, the simplest form of an artificial neuron is a linear function which computes the weighted sum of inputs, to which, optionally, bias can be added:
    y = 􏰁 sum(xi · wi + b) , i = 1:n
    
  • The activation function transforms the result of the summation function (usually) in a non-linear way. If the function is linear, it would simply make the output proportional to the input. We define the activation function as g:
    y = g(sum(􏰁 xi · wi + b)), i = 1:n
    
    The most common non-linear functions used as activation functions include:
    sigmoid function, rectified linear unit (ReLU), hyperbolic tangent.
  • The output is usually the result of an activation function.
  
  2.2.3 Overfitting
  Neural networks are able to learn complicated non-linear functions to fit any training set. On the downside, this may lead to overfitting where the neural network learns the training data so well that it is unable to generalize on new, unseen data. This problem can especially occur on datasets with a small amount of data to learn from.
  In order to prevent the model from overfitting, it is recommended to evaluate it on a separate development set and stop the training once the model stops improving. Typi- cally, if the loss is not decreasing, or on the contrary, it started increasing, then it means that the training needs to be stopped. This can be done through a technique called Early Stopping [Finnoff et al., 1993], which we also apply in our work.
  
  
  
  
  
  
  [Goodfellow et al., 2016] Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press. http://www.deeplearningbook.org.
  [neu, 2018] (2018). Neuron. Wikipedia. https://en.wikipedia.org/wiki/Neuron.
  [McCulloch and Pitts, 1943] McCulloch, W. S. and Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5(4):115–133.
  [Bangal, 2009] Bangal, B. C. (2009). Automatic generation control of interconnected power systems using artificial neural network techniques.
  [Finnoff et al., 1993] Finnoff, W., Hergert, F., and Zimmermann, H.-G. (1993). Ex- tended regularization methods for nonconvergent model selection. In Advances in Neural Information Processing Systems, pages 228–235.
